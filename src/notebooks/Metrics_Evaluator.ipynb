{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba06ee3",
   "metadata": {},
   "source": [
    "# Metrics Evaluator\n",
    "\n",
    "### Introduction\n",
    "\n",
    "To make sure that the splits don't make me choose a wrong model (since the difference of the finalist models is very little), I will run all of them many times to get a consistent metric from them, averaging its scores and stds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9be3c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../configuration.ini']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the necessary modules\n",
    "# Relative paths (make sure you are in src)\n",
    "import os, sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "\n",
    "# Data management libraries \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "import scipy.stats as stats\n",
    "\n",
    "# My code\n",
    "from data_processing import get_path\n",
    "from Models.RandomForest import Random_Forest_ensemble\n",
    "from Models.BaggingEnsemble import Bagging_Ensemble\n",
    "from Models.LinearRegressor import LinearRegressor\n",
    "from Models.LogisticRegressor import LogisticRegressor\n",
    "from Models.LinearRegressionVariants import LR_Relations, LR_ensemble\n",
    "from Models.SVR import SVR_manual\n",
    "from utils import evaluate_classification_metrics, evaluate_regression_metrics, cross_validation, plot_residuals_color\n",
    "from data_processing import standarize_numerical_variables\n",
    "\n",
    "# Machine learning libraries \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor,StackingRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error,mean_squared_error,confusion_matrix, make_scorer\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz, DecisionTreeClassifier\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeCV, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Parameters\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../configuration.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2172955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from relative path for train\n",
    "file = get_path('created_files','train',parent_dir)\n",
    "data = pd.read_csv(file,sep = ',')\n",
    "X_train_p = data.drop(columns=['T3']).reset_index(drop=True)\n",
    "y_train_p = data['T3'].reset_index(drop=True)\n",
    "\n",
    "# Do the train/ test split and standarize AFTER so there is no data leakage\n",
    "X_train, X_test, y_train_unstd, y_test_unstd = train_test_split(X_train_p,y_train_p,test_size=0.2)\n",
    "X_train, X_test, y_train,y_test,scaler_y = standarize_numerical_variables(X_train, X_test, y_train_unstd, y_test_unstd,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d8621",
   "metadata": {},
   "source": [
    "### Model 1: Final Models\n",
    "Models to try out in the Metrics Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36d7fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    #('lr', LinearRegression()),\n",
    "    ('lasso', LassoCV(cv=5)),\n",
    "    ('RF', RandomForestRegressor(random_state=42)),\n",
    "    #('svm', SVR(C=1.0, epsilon=0.2))\n",
    "]\n",
    "stack = StackingRegressor(estimators=base_models, final_estimator=LinearRegression(), cv=5, passthrough=True)\n",
    "models = [LinearRegression(),stack]\n",
    "names = ['LinReg','Stack']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa922816",
   "metadata": {},
   "source": [
    "### Metrics Evaluator Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51249328",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores = []\n",
    "\n",
    "for model in models:\n",
    "    scores = []\n",
    "    stds = []\n",
    "    scores_train = []\n",
    "    MAE = []\n",
    "    MSE = []\n",
    "    for _ in range(300):\n",
    "        X_shuffled, y_shuffled = shuffle(X_train_p, y_train_p, random_state=None)\n",
    "        X_train, X_test, y_train_unstd, y_test_unstd = train_test_split(X_shuffled, y_shuffled, test_size=0.2)\n",
    "        X_train, X_test, y_train, y_test, scaler_y = standarize_numerical_variables(X_train, X_test, y_train_unstd, y_test_unstd, 1)\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        predictions_train = model.predict(X_train)\n",
    "\n",
    "        score = evaluate_regression_metrics(y_test,y_train,predictions,predictions_train)\n",
    "        scores.append(score['R² Score Test:'])\n",
    "        scores_train.append(score['R² Score Train:'])\n",
    "        MAE.append(score['Mean Absolute Error:'])\n",
    "        MSE.append(score['Mean Squared Error:'])\n",
    "\n",
    "    final_scores.append([np.mean(scores),np.std(scores),np.mean(MAE),np.mean(MSE),np.mean(scores_train)])\n",
    "\n",
    "results_df = pd.DataFrame(final_scores, columns=['R² Mean Test', 'R² Std Test', 'Mean Absolute Error', 'Mean Squared Error', 'R² Mean Train'], index=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48c48f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "scores_train = []\n",
    "MAE = []\n",
    "MSE = []\n",
    "for _ in range(300):\n",
    "    X_shuffled, y_shuffled = shuffle(X_train_p, y_train_p, random_state=None)\n",
    "    X_train, X_test, y_train_unstd, y_test_unstd = train_test_split(X_shuffled, y_shuffled, test_size=0.2)\n",
    "    X_train, X_test, y_train, y_test, scaler_y = standarize_numerical_variables(X_train, X_test, y_train_unstd, y_test_unstd, 1)\n",
    "\n",
    "    LR_Rel = LR_Relations()\n",
    "    LR_Rel.fit(X_train,X_test,y_train)\n",
    "    predictions,score = LR_Rel.predict(y_test,y_train)\n",
    "    scores.append(score['R² Score Test:'])\n",
    "    scores_train.append(score['R² Score Train:'])\n",
    "    MAE.append(score['Mean Absolute Error:'])\n",
    "    MSE.append(score['Mean Squared Error:'])\n",
    "\n",
    "final_score = [np.mean(scores),np.std(scores),np.mean(MAE),np.mean(MSE),np.mean(scores_train)]\n",
    "results_df.loc['LR_Relations'] = final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8590f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "scores_train = []\n",
    "MAE = []\n",
    "MSE = []\n",
    "for _ in range(300):\n",
    "    X_shuffled, y_shuffled = shuffle(X_train_p, y_train_p, random_state=None)\n",
    "    X_train, X_test, y_train_unstd, y_test_unstd = train_test_split(X_shuffled, y_shuffled, test_size=0.2)\n",
    "    X_train, X_test, y_train, y_test, scaler_y = standarize_numerical_variables(X_train, X_test, y_train_unstd, y_test_unstd, 1)\n",
    "\n",
    "    LR_E = LR_ensemble()\n",
    "    LR_E.fit(X_train,X_test,y_train)\n",
    "    predictions = LR_E.predict()\n",
    "    score = LR_E.score_complete(y_test,y_train)\n",
    "    scores.append(score['R² Score Test:'])\n",
    "    scores_train.append(score['R² Score Train:'])\n",
    "    MAE.append(score['Mean Absolute Error:'])\n",
    "    MSE.append(score['Mean Squared Error:'])\n",
    "\n",
    "final_score = [np.mean(scores),np.std(scores),np.mean(MAE),np.mean(MSE),np.mean(scores_train)]\n",
    "results_df.loc['LR_Ensemble'] = final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28b6ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              R² Mean Test  R² Std Test  Mean Absolute Error  \\\n",
      "LinReg            0.830102     0.033044             0.251285   \n",
      "LR_Relations      0.833837     0.031910             0.249648   \n",
      "LR_Ensemble       0.833254     0.033965             0.249137   \n",
      "\n",
      "              Mean Squared Error  R² Mean Train  \n",
      "LinReg                  0.169390       0.853024  \n",
      "LR_Relations            0.168190       0.854739  \n",
      "LR_Ensemble             0.166946       0.852139  \n"
     ]
    }
   ],
   "source": [
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
