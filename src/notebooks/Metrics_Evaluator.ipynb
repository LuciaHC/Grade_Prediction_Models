{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba06ee3",
   "metadata": {},
   "source": [
    "# Metrics Evaluator\n",
    "\n",
    "### Introduction\n",
    "\n",
    "To make sure that the splits don't make me choose a wrong model (since the difference of the finalist models is very little), I will run all of them many times to get a consistent metric from them, averaging its scores and stds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9be3c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../configuration.ini']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the necessary modules\n",
    "# Relative paths (make sure you are in src)\n",
    "import os, sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()\n",
    "\n",
    "# Data management libraries \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import math\n",
    "import scipy.stats as stats\n",
    "\n",
    "# My code\n",
    "from data_processing import get_path\n",
    "from Models.RandomForest import Random_Forest_ensemble\n",
    "from Models.BaggingEnsemble import Bagging_Ensemble\n",
    "from Models.LinearRegressor import LinearRegressor\n",
    "from Models.LogisticRegressor import LogisticRegressor\n",
    "from Models.LinearRegressionVariants import LR_Relations, LR_ensemble\n",
    "from Models.SVR import SVR_manual\n",
    "from utils import evaluate_classification_metrics, evaluate_regression_metrics, cross_validation, plot_residuals_color\n",
    "from data_processing import standarize_numerical_variables\n",
    "\n",
    "# Machine learning libraries \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, RandomForestClassifier, RandomForestRegressor,StackingRegressor,GradientBoostingRegressor\n",
    "from sklearn.metrics import accuracy_score, mean_absolute_error,mean_squared_error,confusion_matrix, make_scorer\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz, DecisionTreeClassifier\n",
    "from sklearn.decomposition import KernelPCA, PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, RidgeCV, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import Isomap\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Parameters\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../configuration.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2172955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from relative path for train\n",
    "file = get_path('created_files','train',parent_dir)\n",
    "data = pd.read_csv(file,sep = ',')\n",
    "X_train_p = data.drop(columns=['T3']).reset_index(drop=True)\n",
    "y_train_p = data['T3'].reset_index(drop=True)\n",
    "\n",
    "# Do the train/ test split and standarize AFTER so there is no data leakage\n",
    "X_train, X_test, y_train_unstd, y_test_unstd = train_test_split(X_train_p,y_train_p,test_size=0.2)\n",
    "X_train, X_test, y_train,y_test,scaler_y = standarize_numerical_variables(X_train, X_test, y_train_unstd, y_test_unstd,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d8621",
   "metadata": {},
   "source": [
    "### Model 1: Final Models\n",
    "\n",
    "I will run this code twice, so I will test 600 models. In my paper, I will average as well both runs.\n",
    "\n",
    "Models to try out in the Metrics Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36d7fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    ('lasso', LassoCV(cv=5)),\n",
    "    ('RF', RandomForestRegressor(random_state=42,n_estimators=100)),\n",
    "]\n",
    "stack = StackingRegressor(estimators=base_models, final_estimator=LinearRegression(), cv=5, passthrough=True)\n",
    "rf = RandomForestRegressor(random_state=0,n_estimators=100,max_features=0.8) \n",
    "svr  = SVR(C=1, gamma='scale',kernel='linear')\n",
    "gb = GradientBoostingRegressor(n_estimators=50,max_features=0.8,subsample=0.9,random_state=0)\n",
    "\n",
    "models = [LinearRegression(),stack,BaggingRegressor(random_state=0,n_estimators=100), rf,svr,gb]\n",
    "names = ['LinReg','Stack','Bagging','Random Forest','SVR','Boosting']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa922816",
   "metadata": {},
   "source": [
    "### Metrics Evaluator Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51249328",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores = []\n",
    "\n",
    "for model in models:\n",
    "    scores = []\n",
    "    stds = []\n",
    "    scores_train = []\n",
    "    MAE = []\n",
    "    MSE = []\n",
    "    for _ in range(300):\n",
    "        X_shuffled, y_shuffled = shuffle(X_train_p, y_train_p, random_state=None)\n",
    "        X_train, X_test, y_train_unstd, y_test_unstd = train_test_split(X_shuffled, y_shuffled, test_size=0.2)\n",
    "        X_train, X_test, y_train, y_test, scaler_y = standarize_numerical_variables(X_train, X_test, y_train_unstd, y_test_unstd, 1)\n",
    "\n",
    "        model.fit(X_train,y_train_unstd)\n",
    "        predictions = model.predict(X_test)\n",
    "        predictions_train = model.predict(X_train)\n",
    "\n",
    "        score = evaluate_regression_metrics(y_test_unstd,y_train_unstd,predictions,predictions_train)\n",
    "        scores.append(score['R² Score Test:'])\n",
    "        scores_train.append(score['R² Score Train:'])\n",
    "        MAE.append(score['Mean Absolute Error:'])\n",
    "        MSE.append(score['Mean Squared Error:'])\n",
    "\n",
    "    final_scores.append([np.mean(scores),np.std(scores),np.mean(MAE),np.mean(MSE),np.mean(scores_train)])\n",
    "\n",
    "results_df = pd.DataFrame(final_scores, columns=['R² Mean Test', 'R² Std Test', 'Mean Absolute Error', 'Mean Squared Error', 'R² Mean Train'], index=names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48c48f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "scores_train = []\n",
    "MAE = []\n",
    "MSE = []\n",
    "for _ in range(300):\n",
    "    X_shuffled, y_shuffled = shuffle(X_train_p, y_train_p, random_state=None)\n",
    "    X_train, X_test, y_train_unstd, y_test_unstd = train_test_split(X_shuffled, y_shuffled, test_size=0.2)\n",
    "    X_train, X_test, y_train, y_test, scaler_y = standarize_numerical_variables(X_train, X_test, y_train_unstd, y_test_unstd, 1)\n",
    "\n",
    "    LR_Rel = LR_Relations()\n",
    "    LR_Rel.fit(X_train,X_test,y_train_unstd)\n",
    "    predictions,score = LR_Rel.predict(y_test_unstd,y_train_unstd)\n",
    "    scores.append(score['R² Score Test:'])\n",
    "    scores_train.append(score['R² Score Train:'])\n",
    "    MAE.append(score['Mean Absolute Error:'])\n",
    "    MSE.append(score['Mean Squared Error:'])\n",
    "\n",
    "final_score = [np.mean(scores),np.std(scores),np.mean(MAE),np.mean(MSE),np.mean(scores_train)]\n",
    "results_df.loc['LR_Relations'] = final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8590f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "scores_train = []\n",
    "MAE = []\n",
    "MSE = []\n",
    "for _ in range(300):\n",
    "    X_shuffled, y_shuffled = shuffle(X_train_p, y_train_p, random_state=None)\n",
    "    X_train, X_test, y_train_unstd, y_test_unstd = train_test_split(X_shuffled, y_shuffled, test_size=0.2)\n",
    "    X_train, X_test, y_train, y_test, scaler_y = standarize_numerical_variables(X_train, X_test, y_train_unstd, y_test_unstd, 1)\n",
    "\n",
    "    LR_E = LR_ensemble()\n",
    "    LR_E.fit(X_train,X_test,y_train_unstd)\n",
    "    predictions = LR_E.predict()\n",
    "    score = LR_E.score_complete(y_test_unstd,y_train_unstd)\n",
    "    scores.append(score['R² Score Test:'])\n",
    "    scores_train.append(score['R² Score Train:'])\n",
    "    MAE.append(score['Mean Absolute Error:'])\n",
    "    MSE.append(score['Mean Squared Error:'])\n",
    "\n",
    "final_score = [np.mean(scores),np.std(scores),np.mean(MAE),np.mean(MSE),np.mean(scores_train)]\n",
    "results_df.loc['LR_Ensemble'] = final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "28b6ad31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               R² Mean Test  R² Std Test  Mean Absolute Error  \\\n",
      "LinReg             0.830731     0.032955             0.998602   \n",
      "Stack              0.844978     0.032056             0.970253   \n",
      "Bagging            0.850915     0.034963             0.929575   \n",
      "Random Forest      0.856393     0.034352             0.919295   \n",
      "SVR                0.838306     0.035417             0.899967   \n",
      "Boosting           0.865615     0.031133             0.894528   \n",
      "LR_Relations       0.833887     0.034867             0.998346   \n",
      "LR_Ensemble        0.831899     0.035824             0.991089   \n",
      "\n",
      "               Mean Squared Error  R² Mean Train  \n",
      "LinReg                   2.688406       0.852955  \n",
      "Stack                    2.474833       0.939786  \n",
      "Bagging                  2.317076       0.979435  \n",
      "Random Forest            2.265342       0.979889  \n",
      "SVR                      2.542479       0.838930  \n",
      "Boosting                 2.133524       0.926174  \n",
      "LR_Relations             2.649568       0.854351  \n",
      "LR_Ensemble              2.640029       0.852580  \n"
     ]
    }
   ],
   "source": [
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cc2a93",
   "metadata": {},
   "source": [
    "### Model 2: Final Models\n",
    "Models to try out in the Metrics Evaluator.\n",
    "\n",
    "It is curious to see that the n_estimators that I got from cross-validation is for the 3 ensemble models, exactly twice the number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cbd7897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from relative path for train\n",
    "file = get_path('created_files','train',parent_dir)\n",
    "data = pd.read_csv(file,sep = ',')\n",
    "X_train_p = data.drop(columns=['T3','T2','T1']).reset_index(drop=True)\n",
    "y_train_p = data['T3'].reset_index(drop=True)\n",
    "\n",
    "# Do the train/ test split and standarize AFTER so there is no data leakage\n",
    "X_train, X_test, y_train_unstd, y_test_unstd = train_test_split(X_train_p,y_train_p,test_size=0.2)\n",
    "X_train, X_test, y_train,y_test,scaler_y = standarize_numerical_variables(X_train, X_test, y_train_unstd, y_test_unstd,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    ('gbr', GradientBoostingRegressor(n_estimators=100,max_features=0.8,subsample=0.5,random_state=42)),\n",
    "    ('RF', RandomForestRegressor(random_state=42,n_estimators=200,max_features=0.8) ),\n",
    "    ('bag', BaggingRegressor(random_state=42,n_estimators=200))]\n",
    "\n",
    "stack = StackingRegressor(estimators=base_models, final_estimator=LassoCV(), cv=5, passthrough=True)\n",
    "rf = RandomForestRegressor(random_state=0,n_estimators=200,max_features=0.8) \n",
    "svr  = SVR(C=10, gamma='scale',kernel='rbf')\n",
    "gb = GradientBoostingRegressor(n_estimators=100,max_features=0.8,subsample=0.5,random_state=0)\n",
    "bg = BaggingRegressor(random_state=0,n_estimators=200)\n",
    "\n",
    "models_m2 = [LinearRegression(),stack,bg, rf,svr,gb]\n",
    "names_m2 = ['LinReg','Stack','Bagging','Random Forest','SVR','Boosting']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0030f41",
   "metadata": {},
   "source": [
    "### Metrics Evaluator Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0d43ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_m2 = []\n",
    "\n",
    "for model in models_m2:\n",
    "    scores = []\n",
    "    stds = []\n",
    "    scores_train = []\n",
    "    MAE = []\n",
    "    MSE = []\n",
    "    for _ in range(300):\n",
    "        X_shuffled, y_shuffled = shuffle(X_train_p, y_train_p, random_state=None)\n",
    "        X_train, X_test, y_train_unstd, y_test_unstd = train_test_split(X_shuffled, y_shuffled, test_size=0.2)\n",
    "        X_train, X_test, y_train, y_test, scaler_y = standarize_numerical_variables(X_train, X_test, y_train_unstd, y_test_unstd, 2)\n",
    "\n",
    "        model.fit(X_train,y_train_unstd)\n",
    "        predictions = model.predict(X_test)\n",
    "        predictions_train = model.predict(X_train)\n",
    "\n",
    "        score = evaluate_regression_metrics(y_test_unstd,y_train_unstd,predictions,predictions_train)\n",
    "        scores.append(score['R² Score Test:'])\n",
    "        scores_train.append(score['R² Score Train:'])\n",
    "        MAE.append(score['Mean Absolute Error:'])\n",
    "        MSE.append(score['Mean Squared Error:'])\n",
    "\n",
    "    final_scores_m2.append([np.mean(scores),np.std(scores),np.mean(MAE),np.mean(MSE),np.mean(scores_train)])\n",
    "\n",
    "results_df_m2 = pd.DataFrame(final_scores_m2, columns=['R² Mean Test', 'R² Std Test', 'Mean Absolute Error', 'Mean Squared Error', 'R² Mean Train'], index=names_m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3e669c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               R² Mean Test  R² Std Test  Mean Absolute Error  \\\n",
      "LinReg             0.199626     0.062545             2.593248   \n",
      "Stack              0.315392     0.069937             2.409327   \n",
      "Bagging            0.299883     0.080667             2.395442   \n",
      "Random Forest      0.305675     0.076834             2.395336   \n",
      "SVR                0.285689     0.065760             2.419332   \n",
      "Boosting           0.300555     0.081004             2.470903   \n",
      "\n",
      "               Mean Squared Error  R² Mean Train  \n",
      "LinReg                  12.763015       0.302269  \n",
      "Stack                   10.817083       0.783729  \n",
      "Bagging                 10.971332       0.904835  \n",
      "Random Forest           10.935720       0.905903  \n",
      "SVR                     11.423480       0.730712  \n",
      "Boosting                11.025666       0.681296  \n"
     ]
    }
   ],
   "source": [
    "print(results_df_m2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
